{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTCj6obrOFkE"
      },
      "source": [
        "**Install requirements**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Myb_4jh5OLob",
        "outputId": "f59494cb-5252-4d35-8847-f08faaa371e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting segmentation-models-pytorch\n",
            "  Downloading segmentation_models_pytorch-0.4.0-py3-none-any.whl.metadata (32 kB)\n",
            "Collecting efficientnet-pytorch>=0.6.1 (from segmentation-models-pytorch)\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (1.26.4)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (11.1.0)\n",
            "Collecting pretrainedmodels>=0.7.1 (from segmentation-models-pytorch)\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (1.17.0)\n",
            "Requirement already satisfied: timm>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (1.0.14)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.20.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (4.12.2)\n",
            "Collecting munch (from pretrainedmodels>=0.7.1->segmentation-models-pytorch)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm>=0.9->segmentation-models-pytorch) (0.5.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8->segmentation-models-pytorch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8->segmentation-models-pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8->segmentation-models-pytorch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2024.12.14)\n",
            "Downloading segmentation_models_pytorch-0.4.0-py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16424 sha256=780788bd25d6e50055088140721a163c7d3921de99ce01ec9dc271af09cb91d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/6f/9b/231a832f811ab6ebb1b32455b177ffc6b8b1cd8de19de70c09\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60944 sha256=bd398f0683e3fb11ec35d1323b6ba240d691def581e60b27b974e91a801017ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/5b/96/fd94bc35962d7c6b699e8814db545155ac91d2b95785e1b035\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: munch, efficientnet-pytorch, pretrainedmodels, segmentation-models-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.4.0\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.5.1+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Downloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.9 torchmetrics-1.6.1\n",
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore) (1.26.4)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from fvcore) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from fvcore) (2.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from fvcore) (11.1.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from iopath>=0.1.7->fvcore) (4.12.2)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61396 sha256=df35fdc3be06fa1a959104261d34b0e8157d1b8c82e25fdd345c77405af62771\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/71/95/3b8fde5c65c6e4a806e0867c1651dcc71a1cb2f3430e8f355f\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=f64fadf1949b0165b946b8cec00d1157b6e662a7b9b11401b32f12d81dddc5c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/5e/16/6117f8fe7e9c0c161a795e10d94645ebcf301ccbd01f66d8ec\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.1.1 yacs-0.1.8\n",
            "Collecting ptflops\n",
            "  Downloading ptflops-0.7.4-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from ptflops) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0->ptflops) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->ptflops) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->ptflops) (3.0.2)\n",
            "Downloading ptflops-0.7.4-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: ptflops\n",
            "Successfully installed ptflops-0.7.4\n"
          ]
        }
      ],
      "source": [
        "!pip install segmentation-models-pytorch\n",
        "!pip install torchmetrics\n",
        "!pip install fvcore\n",
        "!pip install ptflops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyBVo7i-VAF_"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xVwEd7OPVCIu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import jaccard_score\n",
        "\n",
        "import time\n",
        "\n",
        "from collections import OrderedDict\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MF2ye8C0L_Q"
      },
      "source": [
        "# **Global variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "33KCrqfd0P63"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = 7\n",
        "\n",
        "BATCH_SIZE = 16     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                    # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = 0.01             # The initial Learning Rate\n",
        "MOMENTUM = 0.9        # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 0.0001 # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = 20      # Total number of training epochs (iterations over dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hQSByUy3PC6"
      },
      "source": [
        "# **Dataset management**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jesBkOmJLLr6"
      },
      "source": [
        "**Download Dataset LoveDA**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5xMezA03LJkG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "667748ff-cf8a-412f-f24c-05d99fdaa9ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#'''\n",
        "#1yzLCHM-BHGaEvt5DqnCBzaLfLF-qOQ7M\n",
        "if not os.path.isdir('/content/LoveDA'):\n",
        "  # Download the LoveDA dataset if it doesn't exist in the directory\n",
        "  !gdown --id 1yzLCHM-BHGaEvt5DqnCBzaLfLF-qOQ7M # Estimated time: 3-5 minutes\n",
        "  !jar xvf  \"/content/LoveDA.zip\" # Extract the file\n",
        "\n",
        "# Check if the dataset was copied correctly\n",
        "if not os.path.isdir('/content/LoveDA'):\n",
        "    print(\"Dataset doesn't exist\")\n",
        "\n",
        "#Weights\n",
        "if not os.path.isfile(\"/content/PIDNet_S_ImageNet.pth.tar\"):\n",
        "  !gdown --id 1reDp2XdraXqPHXvmZvIXzXN8lrTphPk8\n",
        "\n",
        "if not os.path.isfile(\"/content/best_model_pidnet.pth\"):\n",
        "  !gdown --id\n",
        "\n",
        "# Utilities\n",
        "if not os.path.isfile(\"/content/model_utils.py\"):\n",
        "    # Download the model_utils.py script\n",
        "    !gdown --id 1Uq3E3F6fO7qXInkS2IFGHS5WDxrrNSpr\n",
        "'''\n",
        "\n",
        "# OPTION 2\n",
        "# Download LoveDA train an val dataset, from https://zenodo.org/records/5706578, extract and put them in a unique folder: LoveDA\n",
        "# Download the Utils folder from github: https://github.com/Zafonte/Real-time-Domain-Adaptation-in-Semantic-Segmentation.git\n",
        "# Upload in your drive the dataset and the Utils folder - then run the following line of code\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Copy the dataset from your Google Drive folder to /content\n",
        "if not os.path.isdir('/content/LoveDA'):\n",
        "    !cp -r \"/content/drive/MyDrive/AMLProjectCode/LoveDA\" /content/                        #PUT your path\n",
        "\n",
        "# Check if the dataset has been copied correctly\n",
        "if not os.path.isdir('/content/LoveDA'):\n",
        "    print(\"Dataset doesn't exist\")\n",
        "\n",
        "#Weights\n",
        "if not os.path.isfile(\"/content/PIDNet_S_ImageNet.pth.tar\"):\n",
        "  !cp -r \"/content/drive/MyDrive/AMLProjectCode/Utils/PIDNet_S_ImageNet.pth.tar\" /content/  #PUT your path\n",
        "\n",
        "if not os.path.isfile(\"/content/best_model_pidnet.pth\"):\n",
        "  !cp -r \"/content/drive/MyDrive/AMLProjectCode/Utils/best_model_pidnet_Domain_Shift.pth\" /content/     #PUT your path\n",
        "\n",
        "# Utilities\n",
        "if not os.path.isfile(\"/content/model_utils.py\"):\n",
        "    !cp -r \"/content/drive/MyDrive/AMLProjectCode/Utils/model_utils.py\" /content/           #PUT your path\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi6R1l3uVOO3"
      },
      "source": [
        "**LoveDA dataset class**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rrlgVR8XVQiH"
      },
      "outputs": [],
      "source": [
        "COLOR_MAP = OrderedDict(\n",
        "    Black = (0, 0, 0),\n",
        "    Background=(255, 255, 255),\n",
        "    Building=(255, 0, 0),\n",
        "    Road=(255, 255, 0),\n",
        "    Water=(0, 0, 255),\n",
        "    Barren=(159, 129, 183),\n",
        "    Forest=(0, 255, 0),\n",
        "    Agricultural=(255, 195, 128),\n",
        ")\n",
        "\n",
        "\n",
        "LABEL_MAP = OrderedDict(\n",
        "    Black = -1,\n",
        "    Background=0,\n",
        "    Building=1,\n",
        "    Road=2,\n",
        "    Water=3,\n",
        "    Barren=4,\n",
        "    Forest=5,\n",
        "    Agricultural=6\n",
        ")\n",
        "\n",
        "\n",
        "def map_labels(label):\n",
        "    mapped_label = np.where(label == 0, -1, np.where(label >= 1, label - 1, label)) # Change pixel values\n",
        "    return torch.tensor(mapped_label, dtype=torch.long)\n",
        "\n",
        "\n",
        "class LoveDADataset(Dataset):\n",
        "    def __init__(self, images_path, masks_path, image_transform=None, mask_transform=None):\n",
        "        self.images_path = images_path\n",
        "        self.masks_path = masks_path\n",
        "        self.image_transform = image_transform\n",
        "        self.mask_transform = mask_transform\n",
        "        self.images = sorted([f for f in os.listdir(images_path) if f.endswith('.png')])\n",
        "        self.masks = sorted([f for f in os.listdir(masks_path) if f.endswith('.png')])\n",
        "\n",
        "        # Check if the number of images matches the number of masks\n",
        "        if len(self.images) != len(self.masks):\n",
        "            raise ValueError(\"The number of images and masks does not match!\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        '''\n",
        "        __getitem__ should access an element through its index\n",
        "        Args:\n",
        "            index (int): Index\n",
        "\n",
        "        Returns:\n",
        "            tuple: (sample, target)\n",
        "        '''\n",
        "        # Read image and mask\n",
        "        image = Image.open(os.path.join(self.images_path, self.images[idx])).convert('RGB')\n",
        "        mask = Image.open(os.path.join(self.masks_path, self.masks[idx]))\n",
        "\n",
        "        # Apply transformation to images (if present)\n",
        "        if self.image_transform:\n",
        "            image = self.image_transform(image)\n",
        "\n",
        "        # Apply transformation to masks (if present)\n",
        "        if self.mask_transform:\n",
        "            mask = self.mask_transform(mask)\n",
        "\n",
        "        # Convert mask to NumPy array for mapping\n",
        "        mask_np = np.array(mask, dtype=np.int64)\n",
        "\n",
        "        # Map labels using map_labels function\n",
        "        mask_mapped = map_labels(mask_np)\n",
        "\n",
        "        return image, mask_mapped\n",
        "\n",
        "    def __len__(self):\n",
        "        '''\n",
        "        The __len__ method returns the length of the dataset\n",
        "        It is mandatory, as this is used by several other components\n",
        "        '''\n",
        "        return len(self.images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZgs35Kb31Wb"
      },
      "source": [
        "# **Network setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atQjjWg2VFq5"
      },
      "source": [
        "**PIDNet**: a real-time segmentation network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "I1Aq7BboF9ap"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# Written by Jiacong Xu (jiacong.xu@tamu.edu)\n",
        "# ------------------------------------------------------------------------------\n",
        "import sys\n",
        "import os\n",
        "\n",
        "from model_utils import BasicBlock, Bottleneck, segmenthead, DAPPM, PAPPM, PagFM, Bag, Light_Bag\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import logging\n",
        "\n",
        "BatchNorm2d = nn.BatchNorm2d\n",
        "bn_mom = 0.1\n",
        "algc = False\n",
        "\n",
        "class PIDNet(nn.Module):\n",
        "\n",
        "    def __init__(self, m=2, n=3, num_classes=NUM_CLASSES, planes=64, ppm_planes=96, head_planes=128, augment=True):\n",
        "        super(PIDNet, self).__init__()\n",
        "        self.augment = augment\n",
        "\n",
        "        # I Branch - extracts key features at reduced resolutions\n",
        "        self.conv1 =  nn.Sequential(\n",
        "                          nn.Conv2d(3,planes,kernel_size=3, stride=2, padding=1),\n",
        "                          BatchNorm2d(planes, momentum=bn_mom),\n",
        "                          nn.ReLU(inplace=True),\n",
        "                          nn.Conv2d(planes,planes,kernel_size=3, stride=2, padding=1),\n",
        "                          BatchNorm2d(planes, momentum=bn_mom),\n",
        "                          nn.ReLU(inplace=True),\n",
        "                      )\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self._make_layer(BasicBlock, planes, planes, m)\n",
        "        self.layer2 = self._make_layer(BasicBlock, planes, planes * 2, m, stride=2)\n",
        "        self.layer3 = self._make_layer(BasicBlock, planes * 2, planes * 4, n, stride=2)\n",
        "        self.layer4 = self._make_layer(BasicBlock, planes * 4, planes * 8, n, stride=2)\n",
        "        self.layer5 =  self._make_layer(Bottleneck, planes * 8, planes * 8, 2, stride=2)\n",
        "\n",
        "        # P Branch - optimized for scene parsing\n",
        "        self.compression3 = nn.Sequential(\n",
        "                                          nn.Conv2d(planes * 4, planes * 2, kernel_size=1, bias=False),\n",
        "                                          BatchNorm2d(planes * 2, momentum=bn_mom),\n",
        "                                          )\n",
        "\n",
        "        self.compression4 = nn.Sequential(\n",
        "                                          nn.Conv2d(planes * 8, planes * 2, kernel_size=1, bias=False),\n",
        "                                          BatchNorm2d(planes * 2, momentum=bn_mom),\n",
        "                                          )\n",
        "        self.pag3 = PagFM(planes * 2, planes)\n",
        "        self.pag4 = PagFM(planes * 2, planes)\n",
        "\n",
        "        self.layer3_ = self._make_layer(BasicBlock, planes * 2, planes * 2, m)\n",
        "        self.layer4_ = self._make_layer(BasicBlock, planes * 2, planes * 2, m)\n",
        "        self.layer5_ = self._make_layer(Bottleneck, planes * 2, planes * 2, 1)\n",
        "\n",
        "        # D Branch - captures fine details (edges, contours)\n",
        "        if m == 2:\n",
        "            self.layer3_d = self._make_single_layer(BasicBlock, planes * 2, planes)\n",
        "            self.layer4_d = self._make_layer(Bottleneck, planes, planes, 1)\n",
        "            self.diff3 = nn.Sequential(\n",
        "                                        nn.Conv2d(planes * 4, planes, kernel_size=3, padding=1, bias=False),\n",
        "                                        BatchNorm2d(planes, momentum=bn_mom),\n",
        "                                        )\n",
        "            self.diff4 = nn.Sequential(\n",
        "                                     nn.Conv2d(planes * 8, planes * 2, kernel_size=3, padding=1, bias=False),\n",
        "                                     BatchNorm2d(planes * 2, momentum=bn_mom),\n",
        "                                     )\n",
        "            self.spp = PAPPM(planes * 16, ppm_planes, planes * 4)\n",
        "            self.dfm = Light_Bag(planes * 4, planes * 4)\n",
        "        else:\n",
        "            self.layer3_d = self._make_single_layer(BasicBlock, planes * 2, planes * 2)\n",
        "            self.layer4_d = self._make_single_layer(BasicBlock, planes * 2, planes * 2)\n",
        "            self.diff3 = nn.Sequential(\n",
        "                                        nn.Conv2d(planes * 4, planes * 2, kernel_size=3, padding=1, bias=False),\n",
        "                                        BatchNorm2d(planes * 2, momentum=bn_mom),\n",
        "                                        )\n",
        "            self.diff4 = nn.Sequential(\n",
        "                                     nn.Conv2d(planes * 8, planes * 2, kernel_size=3, padding=1, bias=False),\n",
        "                                     BatchNorm2d(planes * 2, momentum=bn_mom),\n",
        "                                     )\n",
        "            self.spp = DAPPM(planes * 16, ppm_planes, planes * 4)\n",
        "            self.dfm = Bag(planes * 4, planes * 4)\n",
        "\n",
        "        self.layer5_d = self._make_layer(Bottleneck, planes * 2, planes * 2, 1)\n",
        "\n",
        "        # Prediction Head\n",
        "        if self.augment:\n",
        "            self.seghead_p = segmenthead(planes * 2, head_planes, num_classes)\n",
        "            self.seghead_d = segmenthead(planes * 2, planes, 1)\n",
        "\n",
        "        self.final_layer = segmenthead(planes * 4, head_planes, num_classes)\n",
        "\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion, momentum=bn_mom),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(inplanes, planes, stride, downsample))\n",
        "        inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            if i == (blocks-1):\n",
        "                layers.append(block(inplanes, planes, stride=1, no_relu=True))\n",
        "            else:\n",
        "                layers.append(block(inplanes, planes, stride=1, no_relu=False))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _make_single_layer(self, block, inplanes, planes, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion, momentum=bn_mom),\n",
        "            )\n",
        "\n",
        "        layer = block(inplanes, planes, stride, downsample, no_relu=True)\n",
        "\n",
        "        return layer\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        width_output = x.shape[-1] // 8\n",
        "        height_output = x.shape[-2] // 8\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(self.layer2(self.relu(x)))\n",
        "        x_ = self.layer3_(x)\n",
        "        x_d = self.layer3_d(x)\n",
        "\n",
        "        x = self.relu(self.layer3(x))\n",
        "        x_ = self.pag3(x_, self.compression3(x))\n",
        "        x_d = x_d + F.interpolate(\n",
        "                        self.diff3(x),\n",
        "                        size=[height_output, width_output],\n",
        "                        mode='bilinear', align_corners=algc)\n",
        "        if self.augment:\n",
        "            temp_p = x_\n",
        "\n",
        "        x = self.relu(self.layer4(x))\n",
        "        x_ = self.layer4_(self.relu(x_))\n",
        "        x_d = self.layer4_d(self.relu(x_d))\n",
        "\n",
        "        x_ = self.pag4(x_, self.compression4(x))\n",
        "        x_d = x_d + F.interpolate(\n",
        "                        self.diff4(x),\n",
        "                        size=[height_output, width_output],\n",
        "                        mode='bilinear', align_corners=algc)\n",
        "        if self.augment:\n",
        "            temp_d = x_d\n",
        "\n",
        "        x_ = self.layer5_(self.relu(x_))\n",
        "        x_d = self.layer5_d(self.relu(x_d))\n",
        "        x = F.interpolate(\n",
        "                        self.spp(self.layer5(x)),\n",
        "                        size=[height_output, width_output],\n",
        "                        mode='bilinear', align_corners=algc)\n",
        "\n",
        "        x_ = self.final_layer(self.dfm(x_, x, x_d))\n",
        "\n",
        "        if self.augment:\n",
        "            x_extra_p = self.seghead_p(temp_p)\n",
        "            x_extra_d = self.seghead_d(temp_d)\n",
        "            return [x_extra_p, x_, x_extra_d]\n",
        "        else:\n",
        "            return x_\n",
        "\n",
        "def get_seg_model(cfg, imgnet_pretrained):\n",
        "\n",
        "    if 's' in cfg.MODEL.NAME:\n",
        "        model = PIDNet(m=2, n=3, num_classes=cfg.DATASET.NUM_CLASSES, planes=32, ppm_planes=96, head_planes=128, augment=False)\n",
        "    elif 'm' in cfg.MODEL.NAME:\n",
        "        model = PIDNet(m=2, n=3, num_classes=cfg.DATASET.NUM_CLASSES, planes=64, ppm_planes=96, head_planes=128, augment=True)\n",
        "    else:\n",
        "        model = PIDNet(m=3, n=4, num_classes=cfg.DATASET.NUM_CLASSES, planes=64, ppm_planes=112, head_planes=256, augment=True)\n",
        "\n",
        "    if imgnet_pretrained:\n",
        "        print('PIDNet-S backbone pretraining on ImageNet loading...')\n",
        "        pretrained_state = torch.load(cfg.MODEL.PRETRAINED, map_location='cpu')['state_dict']\n",
        "        model_dict = model.state_dict()\n",
        "        pretrained_state = {k: v for k, v in pretrained_state.items() if (k in model_dict and v.shape == model_dict[k].shape)}\n",
        "        model_dict.update(pretrained_state)\n",
        "        msg = 'Loaded {} parameters!'.format(len(pretrained_state))\n",
        "        logging.info('Attention!!!')\n",
        "        logging.info(msg)\n",
        "        logging.info('Over!!!')\n",
        "        model.load_state_dict(model_dict, strict = False)\n",
        "        print('PIDNet-S backbone pretraining on ImageNet LOADED')\n",
        "    else:\n",
        "        pretrained_dict = torch.load(cfg.MODEL.PRETRAINED, map_location='cpu')\n",
        "        if 'state_dict' in pretrained_dict:\n",
        "            pretrained_dict = pretrained_dict['state_dict']\n",
        "        model_dict = model.state_dict()\n",
        "        pretrained_dict = {k[6:]: v for k, v in pretrained_dict.items() if (k[6:] in model_dict and v.shape == model_dict[k[6:]].shape)}\n",
        "        msg = 'Loaded {} parameters!'.format(len(pretrained_dict))\n",
        "        logging.info('Attention!!!')\n",
        "        logging.info(msg)\n",
        "        logging.info('Over!!!')\n",
        "        model_dict.update(pretrained_dict)\n",
        "        model.load_state_dict(model_dict, strict = False)\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_pred_model(name, num_classes):\n",
        "\n",
        "    if 's' in name:\n",
        "        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=32, ppm_planes=96, head_planes=128, augment=False)\n",
        "    elif 'm' in name:\n",
        "        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=64, ppm_planes=96, head_planes=128, augment=False)\n",
        "    else:\n",
        "        model = PIDNet(m=3, n=4, num_classes=num_classes, planes=64, ppm_planes=112, head_planes=256, augment=False)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AEvFuCXRvoUb"
      },
      "outputs": [],
      "source": [
        "# Configuration with the path to the pre-trained file\n",
        "class Config:\n",
        "    class MODEL:\n",
        "        NAME = 's'  # Specify the model ('s', 'm', etc.)\n",
        "        PRETRAINED = '/content/PIDNet_S_ImageNet.pth.tar'  # Path to file PIDNet_S pretrainato on ImageNet\n",
        "\n",
        "    class DATASET:\n",
        "        NUM_CLASSES = NUM_CLASSES  # Number of classes in the dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JjV33Pxh98i"
      },
      "source": [
        "**Visualize prediction function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6YPhnI1vdp9Z"
      },
      "outputs": [],
      "source": [
        "def visualize_prediction(image, true_mask, pred_mask):\n",
        "    \"\"\"\n",
        "    Visualizes the original image, true mask, and predicted mask.\n",
        "    Parameters:\n",
        "    - image: Tensor image (C, H, W).\n",
        "    - true_mask: Tensor ground truth mask.\n",
        "    - pred_mask: Tensor predicted mask.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Comparison of true mask and prediction for a sample train image:\")\n",
        "\n",
        "    # Define ImageNet mean and standard deviation\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "    image = image.cpu().numpy().transpose(1, 2, 0)  # Change image shape for matplotlib\n",
        "\n",
        "    # If the image was normalized for the model, we need to de-normalize it\n",
        "    # De-normalize the image\n",
        "    image = image * std + mean  # Inverse of normalization\n",
        "    image = np.clip(image, 0, 1)  # Ensure values are in the range [0, 1]\n",
        "\n",
        "    # Convert image and masks to numpy arrays for visualization\n",
        "    true_mask_np = np.array(true_mask.cpu(), dtype=np.int64)\n",
        "    pred_mask_np = np.array(pred_mask.cpu(), dtype=np.int64)\n",
        "\n",
        "    # Create a colored mask based on LABEL_MAP and COLOR_MAP\n",
        "    colored_true_mask = np.zeros((true_mask_np.shape[0], true_mask_np.shape[1], 3), dtype=np.uint8)\n",
        "    colored_pred_mask = np.zeros((pred_mask_np.shape[0], pred_mask_np.shape[1], 3), dtype=np.uint8)\n",
        "    for class_name, color in COLOR_MAP.items():\n",
        "        class_value = LABEL_MAP[class_name]  # Get the numerical value for the class\n",
        "        colored_true_mask[true_mask_np == class_value] = color  # Apply correct color\n",
        "        colored_pred_mask[pred_mask_np == class_value] = color  # Apply correct color\n",
        "\n",
        "    # Create figure with subplots\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # Original image\n",
        "    ax[0].imshow(image)\n",
        "    ax[0].set_title('Original Image')\n",
        "    ax[0].axis('off')\n",
        "\n",
        "    # True mask\n",
        "    ax[1].imshow(colored_true_mask)\n",
        "    ax[1].set_title('True Mask')\n",
        "    ax[1].axis('off')\n",
        "    print(f\"Unique values (mapped) in true mask: {np.unique(true_mask_np)}\")\n",
        "\n",
        "    # Predicted mask\n",
        "    ax[2].imshow(colored_pred_mask)\n",
        "    ax[2].set_title('Predicted Mask')\n",
        "    ax[2].axis('off')\n",
        "    print(f\"Unique values in predicted mask: {np.unique(pred_mask_np)}\")\n",
        "    print()\n",
        "\n",
        "    legend_patches = [mpatches.Patch(color=np.array(color) / 255.0, label=class_name) for class_name, color in COLOR_MAP.items()]\n",
        "    plt.legend(handles=legend_patches, bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0.)\n",
        "    # Save or show the image\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsEgJpsb0tHR"
      },
      "source": [
        "# **Define Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CWY-Vt8g0yh3"
      },
      "outputs": [],
      "source": [
        "# Image transformation\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256), interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "    transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizzazione - Modelli pre-addestrati su ImageNet (come ResNet101 per DeepLabV2 e PIDNet-S) si aspettano immagini normalizzate\n",
        "])\n",
        "\n",
        "# Mask transformation\n",
        "mask_transform = transforms.Resize((256, 256), interpolation=transforms.InterpolationMode.NEAREST)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V-DG-1u3-hC"
      },
      "source": [
        "# **Dataset object**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jw6Hj4PA1yQJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01456c38-da15-4351-de84-47cc014dabfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images in the Urban train dataset: 1156\n",
            "Number of images in the Rural validation dataset: 992\n"
          ]
        }
      ],
      "source": [
        "train_dataset = LoveDADataset(\n",
        "    \"/content/LoveDA/Train/Urban/images_png\",\n",
        "    \"/content/LoveDA/Train/Urban/masks_png\",\n",
        "    image_transform=image_transform,\n",
        "    mask_transform=mask_transform\n",
        ")\n",
        "\n",
        "print(f\"Number of images in the Urban train dataset: {len(train_dataset)}\")\n",
        "\n",
        "val_dataset = LoveDADataset(\n",
        "    \"/content/LoveDA/Val/Rural/images_png\",\n",
        "    \"/content/LoveDA/Val/Rural/masks_png\",\n",
        "    image_transform=image_transform,\n",
        "    mask_transform=mask_transform\n",
        ")\n",
        "\n",
        "print(f\"Number of images in the Rural validation dataset: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM0h0vEm8BpC"
      },
      "source": [
        "# **Dataloaders**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YEV41L1yVmOQ"
      },
      "outputs": [],
      "source": [
        "#DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2) #batch_size: numero di immagini processate contemporaneamente\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psz9ephdQ0IY"
      },
      "source": [
        "# **Prepare Network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZTDycgjfQ5y2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5774c4dd-5bd4-4bd2-f747-cbc1ee86965c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PIDNet-S backbone pretraining on ImageNet loading...\n",
            "PIDNet-S backbone pretraining on ImageNet LOADED\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-8498e26dc231>:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pretrained_state = torch.load(cfg.MODEL.PRETRAINED, map_location='cpu')['state_dict']\n"
          ]
        }
      ],
      "source": [
        "# Crea l'oggetto di configurazione\n",
        "cfg = Config()\n",
        "\n",
        "# Get the model with pretrained weights from ImageNet\n",
        "model = get_seg_model(cfg, imgnet_pretrained = True)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Acel8XYdVzA_"
      },
      "source": [
        "# **Validation setup**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Label mapping, excluding Black class (index -1)\n",
        "LABEL = {\n",
        "    'Background': 0,\n",
        "    'Building': 1,\n",
        "    'Road': 2,\n",
        "    'Water': 3,\n",
        "    'Barren': 4,\n",
        "    'Forest': 5,\n",
        "    'Agricultural': 6\n",
        "}\n",
        "\n",
        "# Function to compute confusion matrix\n",
        "# a: ground truth, b: predictions, n: number of classes\n",
        "def fast_hist(a, b, n):\n",
        "    \"\"\"\n",
        "    Compute a confusion matrix between classes.\n",
        "\n",
        "    Args:\n",
        "        a (np.array): Ground truth (1D or 2D array).\n",
        "        b (np.array): Prediction (1D or 2D array).\n",
        "        n (int): Number of classes.\n",
        "\n",
        "    Returns:\n",
        "        np.array: Confusion matrix of shape n x n.\n",
        "    \"\"\"\n",
        "    # Create a boolean index for valid values\n",
        "    k = (a >= 0) & (a < n)\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    return np.bincount(n * a[k].astype(int) + b[k], minlength=n ** 2).reshape(n, n)\n",
        "\n",
        "# Function to compute IoU per class\n",
        "def per_class_iou(hist):\n",
        "    epsilon = 1e-5  # Small constant to avoid division by zero\n",
        "    # intersection = values on the diagonal of the confusion matrix\n",
        "    return np.diag(hist) / (hist.sum(1) + hist.sum(0) - np.diag(hist) + epsilon)\n",
        "\n",
        "# Function to compute mIoU on validation set\n",
        "def compute_mIoU(model, val_loader, num_classes, device):\n",
        "    model.eval()\n",
        "    hist = np.zeros((num_classes, num_classes))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in val_loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            outputs_resized = torch.nn.functional.interpolate(\n",
        "                outputs,\n",
        "                size=(256, 256),\n",
        "                mode='bilinear'\n",
        "            )\n",
        "\n",
        "            preds = torch.argmax(outputs_resized, dim=1) # Predicted class\n",
        "\n",
        "            preds = preds.cpu().numpy()\n",
        "            masks = masks.cpu().numpy()\n",
        "\n",
        "            for lt, lp in zip(masks, preds):\n",
        "                hist += fast_hist(lt.flatten(), lp.flatten(), num_classes)\n",
        "\n",
        "    iou = per_class_iou(hist)\n",
        "    mIoU = np.mean(iou)\n",
        "\n",
        "    # Print IoU per class with class names\n",
        "    print(\"IoU per category (%):\")\n",
        "    for class_idx in range(num_classes):\n",
        "        class_name = list(LABEL.keys())[list(LABEL.values()).index(class_idx)]\n",
        "        print(f\" {class_name}: {iou[class_idx] * 100:.2f}%\")\n",
        "    print()\n",
        "\n",
        "    print(f\"mIoU: {mIoU * 100:.2f}%\")\n",
        "    print()\n",
        "    return mIoU"
      ],
      "metadata": {
        "id": "q-VocrvEO9kJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QhsuxhH8d_C"
      },
      "source": [
        "# **Training**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained = True #SET to true if you want use the pretrained weights otherwise False\n",
        "plot_training = False"
      ],
      "metadata": {
        "id": "QTzTLA-lphSH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "gchWv2bF8aTy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f008b57f-2349-4141-b3ff-d1bd217b6647"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model already trained\n"
          ]
        }
      ],
      "source": [
        "#Scheduler\n",
        "def poly_lr_scheduler(optimizer, init_lr, iter, max_iter, power=0.9):\n",
        "    lr = init_lr * (1 - iter / max_iter) ** power\n",
        "    optimizer.param_groups[0]['lr'] = lr\n",
        "    return lr\n",
        "\n",
        "# Optimizer configuration, loss function, and GradScaler\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = -1)\n",
        "\n",
        "# Training loop\n",
        "scaler = torch.amp.GradScaler()\n",
        "num_epochs = NUM_EPOCHS\n",
        "global_iter = 0  # Global iteration counter\n",
        "\n",
        "loss_history = []\n",
        "lr_history = []\n",
        "miou_history = []\n",
        "\n",
        "best_miou = 0.0  # Initialize best mIoU\n",
        "\n",
        "if trained == False:\n",
        "  for epoch in range(num_epochs):\n",
        "      model.train()\n",
        "      total_loss = 0.0\n",
        "\n",
        "      for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
        "          images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          with torch.cuda.amp.autocast():\n",
        "              outputs = model(images)\n",
        "              #Scale the output to the desired size x = torch.nn.functional.interpolate(x, size=(H, W), mode='bilinear')\n",
        "              outputs_resized = torch.nn.functional.interpolate(outputs, size=(256, 256), mode='bilinear')\n",
        "              loss = criterion(outputs_resized, masks) # Compute loss\n",
        "\n",
        "          scaler.scale(loss).backward() # Backpropagation\n",
        "          scaler.step(optimizer)             # Update weights\n",
        "          scaler.update()                     # Update GradScaler\n",
        "\n",
        "          lr = poly_lr_scheduler(optimizer, init_lr=LR, iter=global_iter, max_iter=len(train_loader) * num_epochs)\n",
        "          global_iter += 1\n",
        "\n",
        "          total_loss += loss.item()\n",
        "\n",
        "      avg_loss = total_loss / len(train_loader)\n",
        "      loss_history.append(avg_loss)  # Append to the list\n",
        "      lr_history.append(lr)          # Append learning rate to the list\n",
        "      print()\n",
        "      print(f\"Epoch {epoch + 1}, Learning Rate: {lr:.6f}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "      # Validation every epoch\n",
        "      mIoU = compute_mIoU(model, val_loader, num_classes=NUM_CLASSES, device=device)\n",
        "      miou_history.append(mIoU)\n",
        "      # Save the best model based on mIoU\n",
        "      if mIoU > best_miou:\n",
        "            best_miou = mIoU\n",
        "            torch.save(model.state_dict(), \"best_model_pidnet_DS.pth\")\n",
        "            print(f\"New best model saved with mIoU: {best_miou:.4f}\")\n",
        "\n",
        "      model.eval()\n",
        "      '''Take a sample image and compare prediction'''\n",
        "        # Load a batch of data\n",
        "      sample_image, sample_label = next(iter(train_loader))  # A batch from DataLoader\n",
        "      sample_image = sample_image[0]  # Select a single example\n",
        "      sample_label = sample_label[0]\n",
        "\n",
        "      # Get prediction from model\n",
        "      with torch.no_grad():\n",
        "          sample_image = sample_image.to(device)  # Transfer image to correct device\n",
        "          pred_mask = model(sample_image.unsqueeze(0)) # Add batch dimension\n",
        "          pred_mask_resized = torch.nn.functional.interpolate(\n",
        "              pred_mask,\n",
        "              size=(256, 256),\n",
        "              mode='bilinear'\n",
        "          )\n",
        "          pred_mask = pred_mask_resized.argmax(dim=1).squeeze(0)  # Predicted class\n",
        "\n",
        "      # Visualize prediction\n",
        "      visualize_prediction(sample_image, sample_label, pred_mask)\n",
        "\n",
        "      trained = True\n",
        "      plot_training = True\n",
        "else:\n",
        "  print(\"Model already trained\")\n",
        "  model.load_state_dict(torch.load(\"/content/best_model_pidnet_Domain_Shift.pth\", weights_only=True))\n",
        "  model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Results**"
      ],
      "metadata": {
        "id": "_sJx8x0hVkuG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "X0rKfU3tJpng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dac5b06-e129-4aa8-db5c-970ff09699e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IoU per category (%):\n",
            " Background: 38.85%\n",
            " Building: 28.69%\n",
            " Road: 25.11%\n",
            " Water: 29.30%\n",
            " Barren: 6.82%\n",
            " Forest: 8.96%\n",
            " Agricultural: 17.73%\n",
            "\n",
            "mIoU: 22.21%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(\"/content/best_model_pidnet_Domain_Shift.pth\", weights_only=True))\n",
        "model.to(device)\n",
        "# Compute mIoU\n",
        "mIoU = compute_mIoU(model, val_loader, num_classes=NUM_CLASSES, device=device)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
